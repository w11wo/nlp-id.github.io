{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Indonesian AI Research \ud83c\uddee\ud83c\udde9 We are a team of Machine Learning enthusiasts who love to experiment with the latest state-of-the-art technology and publish the result as an Open-Source product. We built this website to encourage further advancements on the field of all things related to Indonesian Artificial Intelligence. We aim to gather existing models, datasets, and tools which were developed by various parties and share them here. Links Our Projects NLP Resources HuggingFace \ud83e\udd17 Organization Github Organization Contributors Info Feel free to contribute to this project through our Github repository . This project is greatly inspired by NLP For Thai and MasakhaneNLP .","title":"About"},{"location":"#about","text":"","title":"About"},{"location":"#indonesian-ai-research","text":"We are a team of Machine Learning enthusiasts who love to experiment with the latest state-of-the-art technology and publish the result as an Open-Source product. We built this website to encourage further advancements on the field of all things related to Indonesian Artificial Intelligence. We aim to gather existing models, datasets, and tools which were developed by various parties and share them here.","title":"Indonesian AI Research \ud83c\uddee\ud83c\udde9"},{"location":"#links","text":"Our Projects NLP Resources HuggingFace \ud83e\udd17 Organization Github Organization","title":"Links"},{"location":"#contributors","text":"Info Feel free to contribute to this project through our Github repository . This project is greatly inspired by NLP For Thai and MasakhaneNLP .","title":"Contributors"},{"location":"projects/","text":"Projects Indonesian Automatic Speech Recognition using Wav2Vec2 XLS-R by Arun Babu, Changhan Wang, Andros Tjandra, et al. We fine-tuned an Indonesian ASR model using wav2vec2 on the Indonesian common voice dataset. On the test subset of Indonesian Common Voice, our model achieved a Word Error Rate of 14.290. We released our model on the Hub . Multilingual Speech Recognition for Indonesian Languages Multilingual ASR Performance by Cahya Wirawan . We built a Multilingual Speech Recognition model for Indonesian, Javanese and Sundanese. On the test subset of Indonesian Common Voice 7, our model achieved a Word Error Rate of 4.492. We also provide a live demo to test the model. We released our model on the Hub . Indonesian GPT2 and its Applications GPT-2 Illustrated by Jay Alammar . We pre-trained three Indonesian GPT-2 models ( small , medium , and large) on Indonesian subsets of OSCAR, mC4 and Wikipedia (29GB of text data). We then further fine-tuned these models to various applications, such as story generation , academic journal abstract generation , and chatbots . We provide a live demo to test these models. Image Captioning using CLIP and Marian OpenAI CLIP by Radford et al.. We built an Indonesian image captioning model using CLIP as the visual encoder and Marian as the textual decoder on datasets with Indonesian captions. We provide a live demo to test these models.","title":"Projects"},{"location":"projects/#projects","text":"","title":"Projects"},{"location":"projects/#indonesian-automatic-speech-recognition-using-wav2vec2","text":"XLS-R by Arun Babu, Changhan Wang, Andros Tjandra, et al. We fine-tuned an Indonesian ASR model using wav2vec2 on the Indonesian common voice dataset. On the test subset of Indonesian Common Voice, our model achieved a Word Error Rate of 14.290. We released our model on the Hub .","title":"Indonesian Automatic Speech Recognition using Wav2Vec2"},{"location":"projects/#multilingual-speech-recognition-for-indonesian-languages","text":"Multilingual ASR Performance by Cahya Wirawan . We built a Multilingual Speech Recognition model for Indonesian, Javanese and Sundanese. On the test subset of Indonesian Common Voice 7, our model achieved a Word Error Rate of 4.492. We also provide a live demo to test the model. We released our model on the Hub .","title":"Multilingual Speech Recognition for Indonesian Languages"},{"location":"projects/#indonesian-gpt2-and-its-applications","text":"GPT-2 Illustrated by Jay Alammar . We pre-trained three Indonesian GPT-2 models ( small , medium , and large) on Indonesian subsets of OSCAR, mC4 and Wikipedia (29GB of text data). We then further fine-tuned these models to various applications, such as story generation , academic journal abstract generation , and chatbots . We provide a live demo to test these models.","title":"Indonesian GPT2 and its Applications"},{"location":"projects/#image-captioning-using-clip-and-marian","text":"OpenAI CLIP by Radford et al.. We built an Indonesian image captioning model using CLIP as the visual encoder and Marian as the textual decoder on datasets with Indonesian captions. We provide a live demo to test these models.","title":"Image Captioning using CLIP and Marian"},{"location":"resources/","text":"Overview To help democratize Indonesian Artificial Intelligence, we aim to compile a list of Open-Source NLP (Natural Language Processing) models and datasets, sourced primarily from the HuggingFace Model Hub . We categorized them into their languages and their specific task. Available Languages Indonesian ( Bahasa Indonesia ) Javanese ( Bahasa Jawa ) Sundanese ( Bahasa Sunda ) Multilingual ( Multibahasa ) Contributing If you would like to add new models or datasets, feel free to open a Pull Request !","title":"Overview"},{"location":"resources/#overview","text":"To help democratize Indonesian Artificial Intelligence, we aim to compile a list of Open-Source NLP (Natural Language Processing) models and datasets, sourced primarily from the HuggingFace Model Hub . We categorized them into their languages and their specific task.","title":"Overview"},{"location":"resources/#available-languages","text":"Indonesian ( Bahasa Indonesia ) Javanese ( Bahasa Jawa ) Sundanese ( Bahasa Sunda ) Multilingual ( Multibahasa )","title":"Available Languages"},{"location":"resources/#contributing","text":"If you would like to add new models or datasets, feel free to open a Pull Request !","title":"Contributing"},{"location":"resources/indonesian/automatic-speech-recognition/","text":"Automatic Speech Recognition Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text. Models Name Description Author Link Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Artificial Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Common Voice dataset and synthetic voices generated using Artificial Common Voicer, which again based on Google Text To Speech. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the model for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Galuh HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the model for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Indonesian NLP HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the baseline for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. It was trained using the default hyperparamer and for 2x30 epochs. When using this model, make sure that your speech input is sampled at 16kHz. Indonesian NLP HuggingFace Wav2Vec2-Large-XLSR-53-Indonesia Fine-tuned facebook/wav2vec2-large-xlsr-53 in Indonesia using the Common Voice When using this model, make sure that your speech input is sampled at 16kHz. Muhammad Agung Hambali HuggingFace Wav2Vec2-Large-XLSR-53-Indonesia Fine-tuned facebook/wav2vec2-large-xlsr-53 in Indonesia using the Common Voice When using this model, make sure that your speech input is sampled at 16kHz. Muhammad Agung Hambali HuggingFace XLSR-Indonesia Wav2Vec2 fine-tuned on Common Voice ID Test. Samsul Rahmadani HuggingFace Datasets Name Description Author Link Common Voice The Common Voice dataset consists of a unique MP3 and corresponding text file. Many of the 9,283 recorded hours in the dataset also include demographic metadata like age, sex, and accent that can help train the accuracy of speech recognition engines. Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G. HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Indonesian dataset has 40 hours (3.8G) J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Automatic Speech Recognition"},{"location":"resources/indonesian/automatic-speech-recognition/#automatic-speech-recognition","text":"Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text.","title":"Automatic Speech Recognition"},{"location":"resources/indonesian/automatic-speech-recognition/#models","text":"Name Description Author Link Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Artificial Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Common Voice dataset and synthetic voices generated using Artificial Common Voicer, which again based on Google Text To Speech. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian Fine-tuned facebook/wav2vec2-large-xlsr-53 on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the model for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Galuh HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the model for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz. Indonesian NLP HuggingFace Wav2Vec2-Large-XLSR-Indonesian This is the baseline for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset. It was trained using the default hyperparamer and for 2x30 epochs. When using this model, make sure that your speech input is sampled at 16kHz. Indonesian NLP HuggingFace Wav2Vec2-Large-XLSR-53-Indonesia Fine-tuned facebook/wav2vec2-large-xlsr-53 in Indonesia using the Common Voice When using this model, make sure that your speech input is sampled at 16kHz. Muhammad Agung Hambali HuggingFace Wav2Vec2-Large-XLSR-53-Indonesia Fine-tuned facebook/wav2vec2-large-xlsr-53 in Indonesia using the Common Voice When using this model, make sure that your speech input is sampled at 16kHz. Muhammad Agung Hambali HuggingFace XLSR-Indonesia Wav2Vec2 fine-tuned on Common Voice ID Test. Samsul Rahmadani HuggingFace","title":"Models"},{"location":"resources/indonesian/automatic-speech-recognition/#datasets","text":"Name Description Author Link Common Voice The Common Voice dataset consists of a unique MP3 and corresponding text file. Many of the 9,283 recorded hours in the dataset also include demographic metadata like age, sex, and accent that can help train the accuracy of speech recognition engines. Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G. HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Indonesian dataset has 40 hours (3.8G) J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Datasets"},{"location":"resources/indonesian/language-modeling/","text":"Language Modeling Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence. Masked Language Models Name Description Author Link IndoConvBERT Base Model IndoConvBERT is a ConvBERT model pretrained on Indo4B. Akmal HuggingFace Indonesian BERT Base 1.5G (uncased) It is BERT-base model pre-trained with indonesian Wikipedia and indonesian newspapers using a masked language modeling (MLM) objective. This model is uncased. Cahya Wirawan HuggingFace Indonesian BERT Base 522M (uncased) It is BERT-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace Indonesian RoBERTa Base 522M (uncased) It is RoBERTa-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace Indonesian DistilBERT Base (uncased) This model is a distilled version of the Indonesian BERT base model. This model is uncased. This is one of several other language models that have been pre-trained with indonesian datasets. Cahya Wirawan HuggingFace IndoELECTRA IndoELECTRA is a pre-trained language model based on ELECTRA architecture for the Indonesian Language. This model is base version which use electra-base config. Christopher Albert Lorentius HuggingFace Indonesian RoBERTa Base Indonesian RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset. Flax Community HuggingFace Indonesian RoBERTa Large Indonesian RoBERTa Large is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset. Flax Community HuggingFace IndoBERT Base Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Base Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Large Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Large Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Base Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Base Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Large Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Large Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Base (uncased) IndoBERT is the Indonesian version of BERT model. The model was trained using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles, and an Indonesian Web Corpus. IndoLEM HuggingFace IndoBERT (Indonesian BERT Model) IndoBERT is a pre-trained language model based on BERT architecture for the Indonesian Language. This model is base-uncased version which use bert-base config. Sarah Lintang HuggingFace Indo RoBERTa Small Indo RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles. Wilson Wongso HuggingFace Causal/Generative Language Models Name Description Author Link GPT-2 Indonesian Small Kids Stories GPT-2 Indonesian Small Kids Stories is a causal language model based on the OpenAI GPT-2 model. The model was originally the pre-trained GPT2 Small Indonesian model, which was then fine-tuned on Indonesian kids' stories from Room To Read and Let's Read. Bookbot HuggingFace Indonesian GPT2 Small 522M It is GPT2-small model pre-trained with indonesian Wikipedia using a causal language modeling (CLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace GPT2-small-indonesian This is a pretrained model on Indonesian language using a causal language modeling (CLM) objective. The training data used for this model are Indonesian websites of OSCAR, mc4 and Wikipedia. Flax Community HuggingFace GPT2-medium-indonesian This is a pretrained model on Indonesian language using a causal language modeling (CLM) objective. The training data used for this model are Indonesian websites of OSCAR, mc4 and Wikipedia. Flax Community HuggingFace Indonesian GPT-2 finetuned on Indonesian academic journals This is the Indonesian gpt2-small model fine-tuned to abstracts of Indonesian academic journals. All training was done on a TPUv2-8 VM sponsored by TPU Research Cloud. Galuh HuggingFace Indonesian GPT-2-medium finetuned on Indonesian poems This is the Indonesian gpt2-medium model fine-tuned to Indonesian poems. Muhammad Agung Hambali HuggingFace Indonesian GPT-2 finetuned on Indonesian poems This is the Indonesian gpt2-small model fine-tuned to Indonesian poems. Muhammad Agung Hambali HuggingFace Indo GPT-2 Small Indo GPT-2 Small is a language model based on the GPT-2 model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles. Wilson Wongso HuggingFace Datasets Name Description Author Link mC4-ID Indonesian subset of multilingual C4 dataset, filtered using script provided by Clean Italian mC4 . Akmal, Samsul Rahmadani HuggingFace mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OPUS-100 OPUS-100 is English-centric, meaning that all training pairs include English on either the source or target side. The corpus covers 100 languages (including English). Selected the languages based on the volume of parallel data available in OPUS. Biao Zhang and Philip Williams and Ivan Titov and Rico Sennrich HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace Indonesian Newspapers 2018 The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers: Detik, Kompas, Tempo, CNN Indonesia, Sindo, Republika and Poskota. The articles are dated between 1st January 2018 and 20th August 2018 (with few exceptions dated earlier). Feryandi Nurdiantoro HuggingFace Indonesia Puisi Puisi (poem) is an Indonesian poetic form. The dataset contains 7223 Indonesian puisi with its title and author. Ilham Firdausi Putra HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Language Modeling"},{"location":"resources/indonesian/language-modeling/#language-modeling","text":"Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence.","title":"Language Modeling"},{"location":"resources/indonesian/language-modeling/#masked-language-models","text":"Name Description Author Link IndoConvBERT Base Model IndoConvBERT is a ConvBERT model pretrained on Indo4B. Akmal HuggingFace Indonesian BERT Base 1.5G (uncased) It is BERT-base model pre-trained with indonesian Wikipedia and indonesian newspapers using a masked language modeling (MLM) objective. This model is uncased. Cahya Wirawan HuggingFace Indonesian BERT Base 522M (uncased) It is BERT-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace Indonesian RoBERTa Base 522M (uncased) It is RoBERTa-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace Indonesian DistilBERT Base (uncased) This model is a distilled version of the Indonesian BERT base model. This model is uncased. This is one of several other language models that have been pre-trained with indonesian datasets. Cahya Wirawan HuggingFace IndoELECTRA IndoELECTRA is a pre-trained language model based on ELECTRA architecture for the Indonesian Language. This model is base version which use electra-base config. Christopher Albert Lorentius HuggingFace Indonesian RoBERTa Base Indonesian RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset. Flax Community HuggingFace Indonesian RoBERTa Large Indonesian RoBERTa Large is a masked language model based on the RoBERTa model. It was trained on the OSCAR dataset, specifically the unshuffled_deduplicated_id subset. Flax Community HuggingFace IndoBERT Base Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Base Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Large Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Large Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Base Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Base Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Large Model (phase1 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT-Lite Large Model (phase2 - uncased) IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. Indo Benchmark HuggingFace IndoBERT Base (uncased) IndoBERT is the Indonesian version of BERT model. The model was trained using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles, and an Indonesian Web Corpus. IndoLEM HuggingFace IndoBERT (Indonesian BERT Model) IndoBERT is a pre-trained language model based on BERT architecture for the Indonesian Language. This model is base-uncased version which use bert-base config. Sarah Lintang HuggingFace Indo RoBERTa Small Indo RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles. Wilson Wongso HuggingFace","title":"Masked Language Models"},{"location":"resources/indonesian/language-modeling/#causalgenerative-language-models","text":"Name Description Author Link GPT-2 Indonesian Small Kids Stories GPT-2 Indonesian Small Kids Stories is a causal language model based on the OpenAI GPT-2 model. The model was originally the pre-trained GPT2 Small Indonesian model, which was then fine-tuned on Indonesian kids' stories from Room To Read and Let's Read. Bookbot HuggingFace Indonesian GPT2 Small 522M It is GPT2-small model pre-trained with indonesian Wikipedia using a causal language modeling (CLM) objective. This model is uncased: it does not make a difference between indonesia and Indonesia. Cahya Wirawan HuggingFace GPT2-small-indonesian This is a pretrained model on Indonesian language using a causal language modeling (CLM) objective. The training data used for this model are Indonesian websites of OSCAR, mc4 and Wikipedia. Flax Community HuggingFace GPT2-medium-indonesian This is a pretrained model on Indonesian language using a causal language modeling (CLM) objective. The training data used for this model are Indonesian websites of OSCAR, mc4 and Wikipedia. Flax Community HuggingFace Indonesian GPT-2 finetuned on Indonesian academic journals This is the Indonesian gpt2-small model fine-tuned to abstracts of Indonesian academic journals. All training was done on a TPUv2-8 VM sponsored by TPU Research Cloud. Galuh HuggingFace Indonesian GPT-2-medium finetuned on Indonesian poems This is the Indonesian gpt2-medium model fine-tuned to Indonesian poems. Muhammad Agung Hambali HuggingFace Indonesian GPT-2 finetuned on Indonesian poems This is the Indonesian gpt2-small model fine-tuned to Indonesian poems. Muhammad Agung Hambali HuggingFace Indo GPT-2 Small Indo GPT-2 Small is a language model based on the GPT-2 model. It was trained on the latest (late December 2020) Indonesian Wikipedia articles. Wilson Wongso HuggingFace","title":"Causal/Generative Language Models"},{"location":"resources/indonesian/language-modeling/#datasets","text":"Name Description Author Link mC4-ID Indonesian subset of multilingual C4 dataset, filtered using script provided by Clean Italian mC4 . Akmal, Samsul Rahmadani HuggingFace mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OPUS-100 OPUS-100 is English-centric, meaning that all training pairs include English on either the source or target side. The corpus covers 100 languages (including English). Selected the languages based on the volume of parallel data available in OPUS. Biao Zhang and Philip Williams and Ivan Titov and Rico Sennrich HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace Indonesian Newspapers 2018 The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers: Detik, Kompas, Tempo, CNN Indonesia, Sindo, Republika and Poskota. The articles are dated between 1st January 2018 and 20th August 2018 (with few exceptions dated earlier). Feryandi Nurdiantoro HuggingFace Indonesia Puisi Puisi (poem) is an Indonesian poetic form. The dataset contains 7223 Indonesian puisi with its title and author. Ilham Firdausi Putra HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Datasets"},{"location":"resources/indonesian/question-answering/","text":"Question Answering Summary Systems that automatically answers questions posed by humans in a natural language. Models Name Description Author Link IndoBERT-Lite base fine-tuned on Translated SQuAD v2 IndoBERT-Lite trained by Indo Benchmark and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Akmal HuggingFace IndoBERT-Lite-SQuAD base fine-tuned on Full Translated SQuAD v2 IndoBERT-Lite trained by Indo Benchmark and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Akmal HuggingFace SQuAD Bahasa Albert Model Finetuned Albert base language model with translated SQuAD. Based on huseinzol05's Albert Bahasa. Akmal HuggingFace SQuAD IndoBERT-Lite Base Model Fine-tuned IndoBERT-Lite from IndoBenchmark using Translated SQuAD datasets. Akmal HuggingFace IndoBERT Base-Uncased fine-tuned on Translated Squad v2.0 IndoBERT trained by IndoLEM and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Rifky HuggingFace Datasets Name Description Author Link FacQA The goal of the FacQA dataset is to find the answer to a question from a provided short passage from a news article. Each row in the FacQA dataset consists of a question, a short passage, and a label phrase, which can be found inside the corresponding short passage. There are six categories of questions: date, location, name, organization, person, and quantitative. Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa HuggingFace TyDi QA TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses -- such that we expect models performing well on this set to generalize across a large number of the languages in the world. Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki HuggingFace mLAMA This dataset provides the data for mLAMA, a multilingual version of LAMA. Regarding LAMA see https://github.com/facebookresearch/LAMA . For mLAMA the TREx and GoogleRE part of LAMA was considered and machine translated using Google Translate, and the Wikidata and Google Knowledge Graph API. Nora Kassner and Philipp Dufter and Hinrich Sch\u00fctze HuggingFace","title":"Question Answering"},{"location":"resources/indonesian/question-answering/#question-answering","text":"Summary Systems that automatically answers questions posed by humans in a natural language.","title":"Question Answering"},{"location":"resources/indonesian/question-answering/#models","text":"Name Description Author Link IndoBERT-Lite base fine-tuned on Translated SQuAD v2 IndoBERT-Lite trained by Indo Benchmark and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Akmal HuggingFace IndoBERT-Lite-SQuAD base fine-tuned on Full Translated SQuAD v2 IndoBERT-Lite trained by Indo Benchmark and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Akmal HuggingFace SQuAD Bahasa Albert Model Finetuned Albert base language model with translated SQuAD. Based on huseinzol05's Albert Bahasa. Akmal HuggingFace SQuAD IndoBERT-Lite Base Model Fine-tuned IndoBERT-Lite from IndoBenchmark using Translated SQuAD datasets. Akmal HuggingFace IndoBERT Base-Uncased fine-tuned on Translated Squad v2.0 IndoBERT trained by IndoLEM and fine-tuned on Translated SQuAD 2.0 for Q&A downstream task. Rifky HuggingFace","title":"Models"},{"location":"resources/indonesian/question-answering/#datasets","text":"Name Description Author Link FacQA The goal of the FacQA dataset is to find the answer to a question from a provided short passage from a news article. Each row in the FacQA dataset consists of a question, a short passage, and a label phrase, which can be found inside the corresponding short passage. There are six categories of questions: date, location, name, organization, person, and quantitative. Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa HuggingFace TyDi QA TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses -- such that we expect models performing well on this set to generalize across a large number of the languages in the world. Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki HuggingFace mLAMA This dataset provides the data for mLAMA, a multilingual version of LAMA. Regarding LAMA see https://github.com/facebookresearch/LAMA . For mLAMA the TREx and GoogleRE part of LAMA was considered and machine translated using Google Translate, and the Wikidata and Google Knowledge Graph API. Nora Kassner and Philipp Dufter and Hinrich Sch\u00fctze HuggingFace","title":"Datasets"},{"location":"resources/indonesian/summarization/","text":"Text Summarization Summary The task of producing a shorter version of one or several documents that preserves most of the input's meaning. Models Name Description Author Link Indonesian T5 Summarization Base Model t5-base-indonesian-summarization-cased model is based on t5-base-bahasa-summarization-cased by huseinzol05, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian BERT2GPT Summarization Model bert2gpt-indonesian-summarization model is based on cahya/bert-base-indonesian-1.5G and cahya/gpt2-small-indonesian-522Mby cahya, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian BERT2BERT Summarization Model bert2bert-indonesian-summarization model is based on cahya/bert-base-indonesian-1.5G by cahya, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian T5 Summarization Small Model t5-small-indonesian-summarization-cased model is based on t5-small-bahasa-summarization-cased by huseinzol05, finetuned using indosum dataset. Panggi Libersa Jasri Akadol HuggingFace Indonesian T5 Summarization Base Model t5-base-indonesian-summarization-cased model is based on t5-base-bahasa-summarization-cased by huseinzol05, finetuned using indosum dataset. Panggi Libersa Jasri Akadol HuggingFace Datasets Name Description Author Link WikiLingua A large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. Authors extracted article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. Faisal Ladhak, Esin Durmus, Claire Cardie and Kathleen McKeown HuggingFace Liputan6 A large-scale Indonesian summarization dataset. Authors harvested articles from an online news portal, and obtain 215,827 document-summary pairs. Fajri Koto and Jey Han Lau and Timothy Baldwin HuggingFace XLSum A comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md. Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M. Sohel and Shahriyar, Rifat HuggingFace","title":"Text Summarization"},{"location":"resources/indonesian/summarization/#text-summarization","text":"Summary The task of producing a shorter version of one or several documents that preserves most of the input's meaning.","title":"Text Summarization"},{"location":"resources/indonesian/summarization/#models","text":"Name Description Author Link Indonesian T5 Summarization Base Model t5-base-indonesian-summarization-cased model is based on t5-base-bahasa-summarization-cased by huseinzol05, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian BERT2GPT Summarization Model bert2gpt-indonesian-summarization model is based on cahya/bert-base-indonesian-1.5G and cahya/gpt2-small-indonesian-522Mby cahya, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian BERT2BERT Summarization Model bert2bert-indonesian-summarization model is based on cahya/bert-base-indonesian-1.5G by cahya, finetuned using id_liputan6 dataset. Cahya Wirawan HuggingFace Indonesian T5 Summarization Small Model t5-small-indonesian-summarization-cased model is based on t5-small-bahasa-summarization-cased by huseinzol05, finetuned using indosum dataset. Panggi Libersa Jasri Akadol HuggingFace Indonesian T5 Summarization Base Model t5-base-indonesian-summarization-cased model is based on t5-base-bahasa-summarization-cased by huseinzol05, finetuned using indosum dataset. Panggi Libersa Jasri Akadol HuggingFace","title":"Models"},{"location":"resources/indonesian/summarization/#datasets","text":"Name Description Author Link WikiLingua A large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. Authors extracted article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. Faisal Ladhak, Esin Durmus, Claire Cardie and Kathleen McKeown HuggingFace Liputan6 A large-scale Indonesian summarization dataset. Authors harvested articles from an online news portal, and obtain 215,827 document-summary pairs. Fajri Koto and Jey Han Lau and Timothy Baldwin HuggingFace XLSum A comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md. Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M. Sohel and Shahriyar, Rifat HuggingFace","title":"Datasets"},{"location":"resources/indonesian/text-classification/","text":"Text Classification Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU). Models Name Description Author Link Indo RoBERTa Emotion Classifier Indo RoBERTa Emotion Classifier is emotion classifier based on Indo-roberta model. It was trained on the trained on IndoNLU EmoT dataset. The model used was Indo-roberta and was transfer-learned to an emotion classifier model. Based from the IndoNLU bencmark, the model achieve an f1-macro of 72.05%, accuracy of 71.81%, precision of 72.47% and recall of 71.94%. Steven Limcorn HuggingFace Indonesian RoBERTa Base Sentiment Classifier Indonesian RoBERTa Base Sentiment Classifier is a sentiment-text-classification model based on the RoBERTa model. The model was originally the pre-trained Indonesian RoBERTa Base model, which is then fine-tuned on indonlu's SmSA dataset consisting of Indonesian comments and reviews. Wilson Wongso HuggingFace Datasets Name Description Author Link HoASA An aspect-based sentiment analysis dataset consisting of hotel reviews collected from the hotel aggregator platform, AiryRooms. The dataset covers ten different aspects of hotel quality. Similar to the CASA dataset, each review is labeled with a single sentiment label for each aspect. There are four possible sentiment classes for each sentiment label: positive, negative, neutral, and positive-negative. The positivenegative label is given to a review that contains multiple sentiments of the same aspect but for different objects (e.g., cleanliness of bed and toilet). A. N. Azhar, M. L. Khodra, and A. P. Sutiono HuggingFace Indonesian Clickbait Headlines The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers; detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Okezone, Posmetro-Medan, Republika, Sindonews, Tempo, Tribunnews, and Wowkeren. Andika William and Yunita Sari HuggingFace CASA An aspect-based sentiment analysis dataset consisting of around a thousand car reviews collected from multiple Indonesian online automobile platforms. The dataset covers six aspects of car quality. We define the task to be a multi-label classification task, where each label represents a sentiment for a single aspect with three possible values: positive, negative, and neutral. Arfinda Ilmania, Abdurrahman, Samuel Cahyawijaya, Ayu Purwarianti HuggingFace SmSA This sentence-level sentiment analysis dataset is a collection of comments and reviews in Indonesian obtained from multiple online platforms. The text was crawled and then annotated by several Indonesian linguists to construct this dataset. There are three possible sentiments on the SmSA dataset: positive, negative, and neutral. Ayu Purwarianti and Ida Ayu Putu Ari Crisdayanti HuggingFace WReTE The Wiki Revision Edits Textual Entailment dataset consists of 450 sentence pairs constructed from Wikipedia revision history. The dataset contains pairs of sentences and binary semantic relations between the pairs. The data are labeled as entailed when the meaning of the second sentence can be derived from the first one, and not entailed otherwise. Ken Nabila Setya and Rahmad Mahendra HuggingFace EmoT An emotion classification dataset collected from the social media platform Twitter. The dataset consists of around 4000 Indonesian colloquial language tweets, covering five different emotion labels: anger, fear, happy, love, and sadness. Mei Silviana Saputri, Rahmad Mahendra, and Mirna Adriani HuggingFace SentiWS This dataset add sentiment lexicons for 81 languages generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. Chen, Yanqing and Skiena, Steven HuggingFace WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Text Classification"},{"location":"resources/indonesian/text-classification/#text-classification","text":"Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU).","title":"Text Classification"},{"location":"resources/indonesian/text-classification/#models","text":"Name Description Author Link Indo RoBERTa Emotion Classifier Indo RoBERTa Emotion Classifier is emotion classifier based on Indo-roberta model. It was trained on the trained on IndoNLU EmoT dataset. The model used was Indo-roberta and was transfer-learned to an emotion classifier model. Based from the IndoNLU bencmark, the model achieve an f1-macro of 72.05%, accuracy of 71.81%, precision of 72.47% and recall of 71.94%. Steven Limcorn HuggingFace Indonesian RoBERTa Base Sentiment Classifier Indonesian RoBERTa Base Sentiment Classifier is a sentiment-text-classification model based on the RoBERTa model. The model was originally the pre-trained Indonesian RoBERTa Base model, which is then fine-tuned on indonlu's SmSA dataset consisting of Indonesian comments and reviews. Wilson Wongso HuggingFace","title":"Models"},{"location":"resources/indonesian/text-classification/#datasets","text":"Name Description Author Link HoASA An aspect-based sentiment analysis dataset consisting of hotel reviews collected from the hotel aggregator platform, AiryRooms. The dataset covers ten different aspects of hotel quality. Similar to the CASA dataset, each review is labeled with a single sentiment label for each aspect. There are four possible sentiment classes for each sentiment label: positive, negative, neutral, and positive-negative. The positivenegative label is given to a review that contains multiple sentiments of the same aspect but for different objects (e.g., cleanliness of bed and toilet). A. N. Azhar, M. L. Khodra, and A. P. Sutiono HuggingFace Indonesian Clickbait Headlines The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers; detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Okezone, Posmetro-Medan, Republika, Sindonews, Tempo, Tribunnews, and Wowkeren. Andika William and Yunita Sari HuggingFace CASA An aspect-based sentiment analysis dataset consisting of around a thousand car reviews collected from multiple Indonesian online automobile platforms. The dataset covers six aspects of car quality. We define the task to be a multi-label classification task, where each label represents a sentiment for a single aspect with three possible values: positive, negative, and neutral. Arfinda Ilmania, Abdurrahman, Samuel Cahyawijaya, Ayu Purwarianti HuggingFace SmSA This sentence-level sentiment analysis dataset is a collection of comments and reviews in Indonesian obtained from multiple online platforms. The text was crawled and then annotated by several Indonesian linguists to construct this dataset. There are three possible sentiments on the SmSA dataset: positive, negative, and neutral. Ayu Purwarianti and Ida Ayu Putu Ari Crisdayanti HuggingFace WReTE The Wiki Revision Edits Textual Entailment dataset consists of 450 sentence pairs constructed from Wikipedia revision history. The dataset contains pairs of sentences and binary semantic relations between the pairs. The data are labeled as entailed when the meaning of the second sentence can be derived from the first one, and not entailed otherwise. Ken Nabila Setya and Rahmad Mahendra HuggingFace EmoT An emotion classification dataset collected from the social media platform Twitter. The dataset consists of around 4000 Indonesian colloquial language tweets, covering five different emotion labels: anger, fear, happy, love, and sadness. Mei Silviana Saputri, Rahmad Mahendra, and Mirna Adriani HuggingFace SentiWS This dataset add sentiment lexicons for 81 languages generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. Chen, Yanqing and Skiena, Steven HuggingFace WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Datasets"},{"location":"resources/indonesian/text2text-generation/","text":"Text2Text Generation Summary The task of producing a shorter version of one or several documents that preserves most of the input's meaning Models Name Description Author Link Indonesian T5 Small T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Indonesian T5 Base T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Indonesian T5 Large T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Paraphrase Generation with IndoT5 Base IndoT5-base trained on translated PAWS. Akmal HuggingFace mT5 Large ID QGen QA mT5 fine-tuned on SQuAD, XQuad, and Tydiqa Samsul Rahmadani HuggingFace","title":"Text2Text Generation"},{"location":"resources/indonesian/text2text-generation/#text2text-generation","text":"Summary The task of producing a shorter version of one or several documents that preserves most of the input's meaning","title":"Text2Text Generation"},{"location":"resources/indonesian/text2text-generation/#models","text":"Name Description Author Link Indonesian T5 Small T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Indonesian T5 Base T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Indonesian T5 Large T5 (Text-to-Text Transfer Transformer) model pretrained on Indonesian mC4 with extra filtering. This model is pre-trained only and needs to be fine-tuned to be used for specific tasks. Akmal HuggingFace Paraphrase Generation with IndoT5 Base IndoT5-base trained on translated PAWS. Akmal HuggingFace mT5 Large ID QGen QA mT5 fine-tuned on SQuAD, XQuad, and Tydiqa Samsul Rahmadani HuggingFace","title":"Models"},{"location":"resources/indonesian/token-classification/","text":"Token Classification Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging). Models Name Description Author Link Indonesian RoBERTa Base POSP Tagger Indonesian RoBERTa Base POSP Tagger is a part-of-speech token-classification model based on the RoBERTa model. The model was originally the pre-trained Indonesian RoBERTa Base model, which is then fine-tuned on indonlu's POSP dataset consisting of tag-labelled news. Wilson Wongso HuggingFace Datasets Name Description Author Link BaPOS This POS tagging dataset contains about 1000 sentences, collected from the PAN Localization Project. In this dataset, each word is tagged by one of 23 POS tag classes. Data splitting used in this benchmark follows the experimental setting used by Kurniawan and Aji (2018). Arawinda Dinakaramani, Fam Rashel, Andry Luthfi, and Ruli Manurung & Kemal Kurniawan and Alham Fikri Aji HuggingFace POSP This Indonesian part-of-speech tagging (POS) dataset is collected from Indonesian news websites. The dataset consists of around 8000 sentences with 26 POS tags. The POS tag labels follow the Indonesian Association of Computational Linguistics (INACL) POS Tagging Convention. Devin Hoesen and Ayu Purwarianti HuggingFace NERP This NER dataset (Hoesen and Purwarianti, 2018) contains texts collected from several Indonesian news websites. There are five labels available in this dataset, PER (name of person), LOC (name of location), IND (name of product or brand), EVT (name of the event), and FNB (name of food and beverage). Similar to the TermA dataset, the NERP dataset uses the IOB chunking format. Devin Hoesen and Ayu Purwarianti HuggingFace KEPS This keyphrase extraction dataset consists of text from Twitter discussing banking products and services and is written in the Indonesian language. A phrase containing important information is considered a keyphrase. Text may contain one or more keyphrases since important phrases can be located at different positions. The dataset follows the IOB chunking format, which represents the position of the keyphrase. Miftahul Mahfuzh, Sidik Soleman, and Ayu Purwarianti HuggingFace NERGrit This NER dataset is taken from the Grit-ID repository, and the labels are spans in IOB chunking representation. The dataset consists of three kinds of named entity tags, PERSON (name of person), PLACE (name of location), and ORGANIZATION (name of organization). NERGrit Developers HuggingFace WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace TermA This span-extraction dataset is collected from the hotel aggregator platform, AiryRooms. The dataset consists of thousands of hotel reviews, which each contain a span label for aspect and sentiment words representing the opinion of the reviewer on the corresponding aspect. The labels use Inside-Outside-Beginning (IOB) tagging representation with two kinds of tags, aspect and sentiment. Yosef Ardhito Winatmoko, Ali Akbar Septiandri, Arie Pratama Sutiono & Jordhy Fernando, Masayu Leylia Khodra, Ali Akbar Septiandri HuggingFace","title":"Token Classification"},{"location":"resources/indonesian/token-classification/#token-classification","text":"Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging).","title":"Token Classification"},{"location":"resources/indonesian/token-classification/#models","text":"Name Description Author Link Indonesian RoBERTa Base POSP Tagger Indonesian RoBERTa Base POSP Tagger is a part-of-speech token-classification model based on the RoBERTa model. The model was originally the pre-trained Indonesian RoBERTa Base model, which is then fine-tuned on indonlu's POSP dataset consisting of tag-labelled news. Wilson Wongso HuggingFace","title":"Models"},{"location":"resources/indonesian/token-classification/#datasets","text":"Name Description Author Link BaPOS This POS tagging dataset contains about 1000 sentences, collected from the PAN Localization Project. In this dataset, each word is tagged by one of 23 POS tag classes. Data splitting used in this benchmark follows the experimental setting used by Kurniawan and Aji (2018). Arawinda Dinakaramani, Fam Rashel, Andry Luthfi, and Ruli Manurung & Kemal Kurniawan and Alham Fikri Aji HuggingFace POSP This Indonesian part-of-speech tagging (POS) dataset is collected from Indonesian news websites. The dataset consists of around 8000 sentences with 26 POS tags. The POS tag labels follow the Indonesian Association of Computational Linguistics (INACL) POS Tagging Convention. Devin Hoesen and Ayu Purwarianti HuggingFace NERP This NER dataset (Hoesen and Purwarianti, 2018) contains texts collected from several Indonesian news websites. There are five labels available in this dataset, PER (name of person), LOC (name of location), IND (name of product or brand), EVT (name of the event), and FNB (name of food and beverage). Similar to the TermA dataset, the NERP dataset uses the IOB chunking format. Devin Hoesen and Ayu Purwarianti HuggingFace KEPS This keyphrase extraction dataset consists of text from Twitter discussing banking products and services and is written in the Indonesian language. A phrase containing important information is considered a keyphrase. Text may contain one or more keyphrases since important phrases can be located at different positions. The dataset follows the IOB chunking format, which represents the position of the keyphrase. Miftahul Mahfuzh, Sidik Soleman, and Ayu Purwarianti HuggingFace NERGrit This NER dataset is taken from the Grit-ID repository, and the labels are spans in IOB chunking representation. The dataset consists of three kinds of named entity tags, PERSON (name of person), PLACE (name of location), and ORGANIZATION (name of organization). NERGrit Developers HuggingFace WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace TermA This span-extraction dataset is collected from the hotel aggregator platform, AiryRooms. The dataset consists of thousands of hotel reviews, which each contain a span label for aspect and sentiment words representing the opinion of the reviewer on the corresponding aspect. The labels use Inside-Outside-Beginning (IOB) tagging representation with two kinds of tags, aspect and sentiment. Yosef Ardhito Winatmoko, Ali Akbar Septiandri, Arie Pratama Sutiono & Jordhy Fernando, Masayu Leylia Khodra, Ali Akbar Septiandri HuggingFace","title":"Datasets"},{"location":"resources/indonesian/translation/","text":"Translation Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. Models Name Description Author Link OPUS-MT-ID-EN Machine translation from Indonesian to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-ID Machine translation from English to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ES-ID Machine translation from Spanish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-ES Machine translation from Indonesian to Spanish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-FR-ID Machine translation from French to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-FI Machine translation from Indonesian to Finnish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-FI-ID Machine translation from Finnish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-SV Machine translation from Indonesian to Swedish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-FR Machine translation from Indonesian to French. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-SV-ID Machine translation from Swedish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace mT5-Translate-EN-ID mT5 machine translation from English to Indonesian. Samsul Rahmadani HuggingFace Datasets Name Description Author Link Parallel Text Corpora for Multi-Domain Translation System Parallel Text Corpora for Multi-Domain Translation System created by BPPT (Indonesian Agency for the Assessment and Application of Technology) for PAN Localization Project (A Regional Initiative to Develop Local Language Computing Capacity in Asia). The dataset contains around 24K sentences divided in 4 difference topics (Economic, international, Science and Technology and Sport). Budiono, Hammam Riza, Chairil Hakim HuggingFace Bible Para This is a multilingual parallel corpus created from translations of the Bible. Christos Christodoulopoulos and Mark Steedman HuggingFace KDE4 A parallel corpus of KDE4 localization files. J. Tiedemann HuggingFace Gnome A parallel corpus of GNOME localization files. J. Tiedemann HuggingFace Ubuntu A parallel corpus of Ubuntu localization files. J. Tiedemann HuggingFace Tanzil This is a collection of Quran translations compiled by the Tanzil project. J. Tiedemann HuggingFace Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace Microsoft Terminology Collection The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products. It can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary for language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange. Microsoft & Leo Zhao and Quentin Lhoest HuggingFace Open Subtitles This is a new collection of translated movie subtitles from here . P. Lison and J. Tiedemann HuggingFace QED The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform. Qatar Computing Research Institute, Arabic Language Technologies Group HuggingFace Asian Language Treebank (ALT) The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. Riza, Hammam and Purwoadi, Michael and Uliniansyah, Teduh and Ti, Aw Ai and Aljunied, Sharifah Mahani and Mai, Luong Chi and Thang, Vu Tat and Thai, Nguyen Phuong and Chea, Vichet and Sam, Sethserey and others HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace Web Inventory of Transcribed & Translated (WIT) Ted Talks The Web Inventory Talk is a collection of the original Ted talks and their translated version. The translations are available in more than 109+ languages, though the distribution is not uniform. Cettolo, Mauro and Girardi, Christian and Federico, Marcello HuggingFace","title":"Translation"},{"location":"resources/indonesian/translation/#translation","text":"Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language.","title":"Translation"},{"location":"resources/indonesian/translation/#models","text":"Name Description Author Link OPUS-MT-ID-EN Machine translation from Indonesian to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-ID Machine translation from English to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ES-ID Machine translation from Spanish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-ES Machine translation from Indonesian to Spanish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-FR-ID Machine translation from French to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-FI Machine translation from Indonesian to Finnish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-FI-ID Machine translation from Finnish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-SV Machine translation from Indonesian to Swedish. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-ID-FR Machine translation from Indonesian to French. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-SV-ID Machine translation from Swedish to Indonesian. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace mT5-Translate-EN-ID mT5 machine translation from English to Indonesian. Samsul Rahmadani HuggingFace","title":"Models"},{"location":"resources/indonesian/translation/#datasets","text":"Name Description Author Link Parallel Text Corpora for Multi-Domain Translation System Parallel Text Corpora for Multi-Domain Translation System created by BPPT (Indonesian Agency for the Assessment and Application of Technology) for PAN Localization Project (A Regional Initiative to Develop Local Language Computing Capacity in Asia). The dataset contains around 24K sentences divided in 4 difference topics (Economic, international, Science and Technology and Sport). Budiono, Hammam Riza, Chairil Hakim HuggingFace Bible Para This is a multilingual parallel corpus created from translations of the Bible. Christos Christodoulopoulos and Mark Steedman HuggingFace KDE4 A parallel corpus of KDE4 localization files. J. Tiedemann HuggingFace Gnome A parallel corpus of GNOME localization files. J. Tiedemann HuggingFace Ubuntu A parallel corpus of Ubuntu localization files. J. Tiedemann HuggingFace Tanzil This is a collection of Quran translations compiled by the Tanzil project. J. Tiedemann HuggingFace Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace Microsoft Terminology Collection The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products. It can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary for language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange. Microsoft & Leo Zhao and Quentin Lhoest HuggingFace Open Subtitles This is a new collection of translated movie subtitles from here . P. Lison and J. Tiedemann HuggingFace QED The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform. Qatar Computing Research Institute, Arabic Language Technologies Group HuggingFace Asian Language Treebank (ALT) The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. Riza, Hammam and Purwoadi, Michael and Uliniansyah, Teduh and Ti, Aw Ai and Aljunied, Sharifah Mahani and Mai, Luong Chi and Thang, Vu Tat and Thai, Nguyen Phuong and Chea, Vichet and Sam, Sethserey and others HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace Web Inventory of Transcribed & Translated (WIT) Ted Talks The Web Inventory Talk is a collection of the original Ted talks and their translated version. The translations are available in more than 109+ languages, though the distribution is not uniform. Cettolo, Mauro and Girardi, Christian and Federico, Marcello HuggingFace","title":"Datasets"},{"location":"resources/javanese/automatic-speech-recognition/","text":"Automatic Speech Recognition Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text. Models Name Description Author Link Wav2Vec2-Large-XLSR-Javanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on the OpenSLR High quality TTS data for Javanese. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Datasets Name Description Author Link OpenSLR This data set contains transcribed audio data for Javanese (~185K utterances). The data set consists of wave files, and a TSV file. The file utt_spk_text.tsv contains a FileID, UserID and the transcription of audio in the file. Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Javanese dataset has 53 hours (5.0G). J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Automatic Speech Recognition"},{"location":"resources/javanese/automatic-speech-recognition/#automatic-speech-recognition","text":"Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text.","title":"Automatic Speech Recognition"},{"location":"resources/javanese/automatic-speech-recognition/#models","text":"Name Description Author Link Wav2Vec2-Large-XLSR-Javanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on the OpenSLR High quality TTS data for Javanese. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace","title":"Models"},{"location":"resources/javanese/automatic-speech-recognition/#datasets","text":"Name Description Author Link OpenSLR This data set contains transcribed audio data for Javanese (~185K utterances). The data set consists of wave files, and a TSV file. The file utt_spk_text.tsv contains a FileID, UserID and the transcription of audio in the file. Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Javanese dataset has 53 hours (5.0G). J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Datasets"},{"location":"resources/javanese/language-modeling/","text":"Language Modeling Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence. Masked Language Models Name Description Author Link Javanese BERT Small Javanese BERT Small is a masked language model based on the BERT model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese DistilBERT Small Javanese DistilBERT Small is a masked language model based on the DistilBERT model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese RoBERTa Small Javanese RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese BERT Small IMDB Javanese BERT Small IMDB is a masked language model based on the BERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese DistilBERT Small IMDB Javanese DistilBERT Small IMDB is a masked language model based on the DistilBERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese RoBERTa Small IMDB Javanese RoBERTa Small IMDB is a masked language model based on the RoBERTa model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Causal/Generative Language Models Name Description Author Link Javanese GPT-2 Small Javanese GPT-2 Small is a language model based on the GPT-2 model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese GPT-2 Small IMDB Javanese GPT-2 Small IMDB is a causal language model based on the GPT-2 model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Datasets Name Description Author Link mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Language Modeling"},{"location":"resources/javanese/language-modeling/#language-modeling","text":"Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence.","title":"Language Modeling"},{"location":"resources/javanese/language-modeling/#masked-language-models","text":"Name Description Author Link Javanese BERT Small Javanese BERT Small is a masked language model based on the BERT model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese DistilBERT Small Javanese DistilBERT Small is a masked language model based on the DistilBERT model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese RoBERTa Small Javanese RoBERTa Small is a masked language model based on the RoBERTa model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese BERT Small IMDB Javanese BERT Small IMDB is a masked language model based on the BERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese DistilBERT Small IMDB Javanese DistilBERT Small IMDB is a masked language model based on the DistilBERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese RoBERTa Small IMDB Javanese RoBERTa Small IMDB is a masked language model based on the RoBERTa model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace","title":"Masked Language Models"},{"location":"resources/javanese/language-modeling/#causalgenerative-language-models","text":"Name Description Author Link Javanese GPT-2 Small Javanese GPT-2 Small is a language model based on the GPT-2 model. It was trained on the latest (late December 2020) Javanese Wikipedia articles. Wilson Wongso HuggingFace Javanese GPT-2 Small IMDB Javanese GPT-2 Small IMDB is a causal language model based on the GPT-2 model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace","title":"Causal/Generative Language Models"},{"location":"resources/javanese/language-modeling/#datasets","text":"Name Description Author Link mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Datasets"},{"location":"resources/javanese/text-classification/","text":"Text Classification Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU). Models Name Description Author Link Javanese BERT Small IMDB Classifier Javanese BERT Small IMDB Classifier is a movie-classification model based on the BERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese DistilBERT Small IMDB Classifier Javanese DistilBERT Small IMDB Classifier is a movie-classification model based on the DistilBERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese GPT-2 Small IMDB Classifier Javanese GPT-2 Small IMDB Classifier is a movie-classification model based on the GPT-2 model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese RoBERTa Small IMDB Classifier Javanese RoBERTa Small IMDB Classifier is a movie-classification model based on the RoBERTa model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Datasets Name Description Author Link IMDb Javanese Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. Wilson Wongso & Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher HuggingFace WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Text Classification"},{"location":"resources/javanese/text-classification/#text-classification","text":"Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU).","title":"Text Classification"},{"location":"resources/javanese/text-classification/#models","text":"Name Description Author Link Javanese BERT Small IMDB Classifier Javanese BERT Small IMDB Classifier is a movie-classification model based on the BERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese DistilBERT Small IMDB Classifier Javanese DistilBERT Small IMDB Classifier is a movie-classification model based on the DistilBERT model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese GPT-2 Small IMDB Classifier Javanese GPT-2 Small IMDB Classifier is a movie-classification model based on the GPT-2 model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace Javanese RoBERTa Small IMDB Classifier Javanese RoBERTa Small IMDB Classifier is a movie-classification model based on the RoBERTa model. It was trained on Javanese IMDB movie reviews. Wilson Wongso HuggingFace","title":"Models"},{"location":"resources/javanese/text-classification/#datasets","text":"Name Description Author Link IMDb Javanese Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. Wilson Wongso & Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher HuggingFace WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Datasets"},{"location":"resources/javanese/token-classification/","text":"Token Classification Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging). Datasets Name Description Author Link WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace","title":"Token Classification"},{"location":"resources/javanese/token-classification/#token-classification","text":"Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging).","title":"Token Classification"},{"location":"resources/javanese/token-classification/#datasets","text":"Name Description Author Link WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace","title":"Datasets"},{"location":"resources/javanese/translation/","text":"Translation Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. Models Name Description Author Link OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace Datasets Name Description Author Link Ubuntu A parallel corpus of Ubuntu localization files. J. Tiedemann HuggingFace Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace QED The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform. Qatar Computing Research Institute, Arabic Language Technologies Group HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace","title":"Translation"},{"location":"resources/javanese/translation/#translation","text":"Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language.","title":"Translation"},{"location":"resources/javanese/translation/#models","text":"Name Description Author Link OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace","title":"Models"},{"location":"resources/javanese/translation/#datasets","text":"Name Description Author Link Ubuntu A parallel corpus of Ubuntu localization files. J. Tiedemann HuggingFace Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace QED The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform. Qatar Computing Research Institute, Arabic Language Technologies Group HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace","title":"Datasets"},{"location":"resources/multilingual/automatic-speech-recognition/","text":"Automatic Speech Recognition Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text. Models Name Description Author Link Multilingual Speech Recognition for Indonesian Languages This is the model built for the project Multilingual Speech Recognition for Indonesian Languages. It is a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset, High-quality TTS data for Javanese - SLR41, and High-quality TTS data for Sundanese - SLR44 datasets. Indonesian NLP HuggingFace","title":"Automatic Speech Recognition"},{"location":"resources/multilingual/automatic-speech-recognition/#automatic-speech-recognition","text":"Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text.","title":"Automatic Speech Recognition"},{"location":"resources/multilingual/automatic-speech-recognition/#models","text":"Name Description Author Link Multilingual Speech Recognition for Indonesian Languages This is the model built for the project Multilingual Speech Recognition for Indonesian Languages. It is a fine-tuned facebook/wav2vec2-large-xlsr-53 model on the Indonesian Common Voice dataset, High-quality TTS data for Javanese - SLR41, and High-quality TTS data for Sundanese - SLR44 datasets. Indonesian NLP HuggingFace","title":"Models"},{"location":"resources/sundanese/automatic-speech-recognition/","text":"Automatic Speech Recognition Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text. Models Name Description Author Link Wav2Vec2-Large-XLSR-Sundanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on the OpenSLR High quality TTS data for Sundanese. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace Datasets Name Description Author Link OpenSLR This data set contains transcribed audio data for Sundanese (~220K utterances). The data set consists of wave files, and a TSV file. The file utt_spk_text.tsv contains a FileID, UserID and the transcription of audio in the file. Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Sundanese dataset has 64 hours (6.2G) J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Automatic Speech Recognition"},{"location":"resources/sundanese/automatic-speech-recognition/#automatic-speech-recognition","text":"Summary An interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text.","title":"Automatic Speech Recognition"},{"location":"resources/sundanese/automatic-speech-recognition/#models","text":"Name Description Author Link Wav2Vec2-Large-XLSR-Sundanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on the OpenSLR High quality TTS data for Sundanese. When using this model, make sure that your speech input is sampled at 16kHz. Cahya Wirawan HuggingFace","title":"Models"},{"location":"resources/sundanese/automatic-speech-recognition/#datasets","text":"Name Description Author Link OpenSLR This data set contains transcribed audio data for Sundanese (~220K utterances). The data set consists of wave files, and a TSV file. The file utt_spk_text.tsv contains a FileID, UserID and the transcription of audio in the file. Oddur Kjartansson and Supheakmungkol Sarin and Knot Pipatsrisawat and Martin Jansche and Linne Ha HuggingFace VolLingua107 VoxLingua107 is a speech dataset for training spoken language identification models. The dataset consists of speech segments extracted from YouTube videos & post-processed. The Sundanese dataset has 64 hours (6.2G) J\u00f6rgen Valk, Tanel Alum\u00e4e bark.phon.ioc.ee","title":"Datasets"},{"location":"resources/sundanese/language-modeling/","text":"Language Modeling Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence. Models Name Description Author Link Sundanese GPT-2 Base Sundanese GPT-2 Base is a causal language model based on the OpenAI GPT-2 model. It was trained on four datasets: OSCAR's unshuffled_deduplicated_su subset, the Sundanese mC4 subset, the Sundanese CC100 subset, and Sundanese Wikipedia. Wilson Wongso HuggingFace Sundanese RoBERTa Base Sundanese RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on four datasets: OSCAR's unshuffled_deduplicated_su subset, the Sundanese mC4 subset, the Sundanese CC100 subset, and Sundanese Wikipedia. Wilson Wongso HuggingFace Datasets Name Description Author Link mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Language Modeling"},{"location":"resources/sundanese/language-modeling/#language-modeling","text":"Summary Models that compute probability of a sentence (sequence of words) or the probability of a next word in a sequence.","title":"Language Modeling"},{"location":"resources/sundanese/language-modeling/#models","text":"Name Description Author Link Sundanese GPT-2 Base Sundanese GPT-2 Base is a causal language model based on the OpenAI GPT-2 model. It was trained on four datasets: OSCAR's unshuffled_deduplicated_su subset, the Sundanese mC4 subset, the Sundanese CC100 subset, and Sundanese Wikipedia. Wilson Wongso HuggingFace Sundanese RoBERTa Base Sundanese RoBERTa Base is a masked language model based on the RoBERTa model. It was trained on four datasets: OSCAR's unshuffled_deduplicated_su subset, the Sundanese mC4 subset, the Sundanese CC100 subset, and Sundanese Wikipedia. Wilson Wongso HuggingFace","title":"Models"},{"location":"resources/sundanese/language-modeling/#datasets","text":"Name Description Author Link mC4-sampling This dataset builds upon the AllenAI version of the original mC4 and adds sampling methods to perform perplexity-based filtering on the fly. Please, refer to BERTIN Project. BERTIN Project & Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace mC4 A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \" https://commoncrawl.org \". Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu HuggingFace OSCAR OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. Data is distributed by language in both original and deduplicated form. Ortiz Su\u00e1rez, Pedro Javier and Romary, Laurent and Sagot, Benoit HuggingFace CC100 This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm\u00e1n, Francisco and Joulin, Armand and Grave, Edouard HuggingFace Wikipedia Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump ( https://dumps.wikimedia.org/ ) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.). Wikimedia Foundation HuggingFace","title":"Datasets"},{"location":"resources/sundanese/text-classification/","text":"Text Classification Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU). Models Name Description Author Link Sundanese GPT-2 Base Emotion Classifier Sundanese GPT-2 Base Emotion Classifier is an emotion-text-classification model based on the OpenAI GPT-2 model. The model was originally the pre-trained Sundanese GPT-2 Base model, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace Sundanese BERT Base Emotion Classifier Sundanese BERT Base Emotion Classifier is an emotion-text-classification model based on the BERT model. The model was originally the pre-trained Sundanese BERT Base Uncased model trained by @luche, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace Sundanese RoBERTa Base Emotion Classifier Sundanese RoBERTa Base Emotion Classifier is an emotion-text-classification model based on the RoBERTa model. The model was originally the pre-trained Sundanese RoBERTa Base model, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace Datasets Name Description Author Link WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Text Classification"},{"location":"resources/sundanese/text-classification/#text-classification","text":"Summary The process of labeling, organizing, or categorizing text data into groups. It forms a fundamental part of Natural Language Understanding (NLU).","title":"Text Classification"},{"location":"resources/sundanese/text-classification/#models","text":"Name Description Author Link Sundanese GPT-2 Base Emotion Classifier Sundanese GPT-2 Base Emotion Classifier is an emotion-text-classification model based on the OpenAI GPT-2 model. The model was originally the pre-trained Sundanese GPT-2 Base model, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace Sundanese BERT Base Emotion Classifier Sundanese BERT Base Emotion Classifier is an emotion-text-classification model based on the BERT model. The model was originally the pre-trained Sundanese BERT Base Uncased model trained by @luche, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace Sundanese RoBERTa Base Emotion Classifier Sundanese RoBERTa Base Emotion Classifier is an emotion-text-classification model based on the RoBERTa model. The model was originally the pre-trained Sundanese RoBERTa Base model, which is then fine-tuned on the Sundanese Twitter dataset, consisting of Sundanese tweets. Wilson Wongso HuggingFace","title":"Models"},{"location":"resources/sundanese/text-classification/#datasets","text":"Name Description Author Link WiLI-2018 WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages. The dataset is balanced and a train-test split is provided. Thoma, Martin HuggingFace","title":"Datasets"},{"location":"resources/sundanese/token-classification/","text":"Token Classification Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging). Datasets Name Description Author Link WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace","title":"Token Classification"},{"location":"resources/sundanese/token-classification/#token-classification","text":"Summary The Token classification task is similar to text classification, except each token within the text receives a prediction. A common use of this task is Named Entity Recognition (NER) and Part-of-Speech Tagging (POS Tagging).","title":"Token Classification"},{"location":"resources/sundanese/token-classification/#datasets","text":"Name Description Author Link WikiANN WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng & Rahimi, Afshin and Li, Yuan and Cohn, Trevor HuggingFace","title":"Datasets"},{"location":"resources/sundanese/translation/","text":"Translation Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. Models Name Description Author Link OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace Datasets Name Description Author Link Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace","title":"Translation"},{"location":"resources/sundanese/translation/#translation","text":"Summary The task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language.","title":"Translation"},{"location":"resources/sundanese/translation/#models","text":"Name Description Author Link OPUS-MT-MUL-EN Machine translation from multiple languages to English. Language Technology Research Group at the University of Helsinki HuggingFace OPUS-MT-EN-MUL Machine translation from English to multiple languages. Language Technology Research Group at the University of Helsinki HuggingFace","title":"Models"},{"location":"resources/sundanese/translation/#datasets","text":"Name Description Author Link Tatoeba This is a collection of translated sentences from Tatoeba . J. Tiedemann HuggingFace The Universal Declaration of Human Rights (UDHR) The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General Assembly in Paris on 10 December 1948 during its 183rd plenary meeting. UDHR & Joe Davison HuggingFace","title":"Datasets"}]}